---
title: Introduction
engine: julia
---

MixedModels.jl is part of the JuliaStats ecosystem and so shares a number of interface and design elements with GLM.jl.
MixedModels.jl can also be viewed as the next step in the research programme behind the R package lme4 (and further back, nlme).
The focus of development is on linear mixed effects models with unconstrained covariance matrices, with a secondary focus on generalized linear mixed effects models.
We'll come back to this focus later, when we discuss limitations of the software. For now, let us start off with a simple mixed model.

# A first model

MixedModels.jl ships with a number of sample and testing datasets, with varying levels of documentation.
```{julia}
using MixedModels
MixedModels.datasets()
```

Let's take a look at `insteval`, which is the same dataset [documented in lme4](https://rdrr.io/cran/lme4/man/InstEval.html).

```{julia}
insteval = MixedModels.dataset("insteval")
```

- `s`: individual students (2972 values)
- `d`: individual instructors (from *Dozent*, 2160 values)
- `studage`: student "age" measured in their current semester number
- `lectage`: lecture "age", measuring how many semesters back the lecture rated had taken place (this was part of a retrospective study, so some ratings were from a few years back)
- `service`: whether or not the lecture is held as a "service" in a different department
- `dept`: department (15 unique values)

```{julia}
fm1 = fit(MixedModel,
          @formula(y ~ 1 + studage + lectage + service + (1|s) + (1|d) + (1|dept)),
          insteval; progress=false)
```

Note that we don't need to convert the sample dataset into a DataFrame: there is a standard for tabular data in Julia and MixedModels.jl can consume any table meeting that standard. This can be very useful for large datasets, when having an additional copy of the data in memory can be costly, and also allows for using memory mapped tabular structures.

For display in the Quarto notebook, we set `progress=false`, but the progress meter defaults to enabled. For a given model, you may nonetheless not see the progress meter if the model is finished fitting before the first progress update would be delivered.

# Examining model output

We note that MixedModels.jl has a different display in the REPL than it does in an Markdown/HTML/LaTeX document. Packages are free to define `show` methods for displaying their output different for different output venues (i.e. for different MIME types). We can force the plaintext output with `println`:

```{julia}
println(fm1)
```

The default pretty printing method for HTML output shows less information than the default output in the REPL, but represents a compact way to display many elements in a single table. We can also extract the individual elements:

```{julia}
println(coeftable(fm1)) # the fixed effects
```
```{julia}
println(VarCorr(fm1)) # the variance-covariance, i.e. random effect, estimates
```

Notably, each of these components also has a pretty printing method defined for rich displays.

## Optimization results

For more technically oriented users and debugging problematic models, the fitted model also includes information about its fit:

```{julia}
fm1.optsum
```

## Best linear unbiased predictions

We can also examine the best linear unbiased predictions (BLUPS, i.e. conditional modes) of the fitted model:

```{julia}
# gives a compact mathematical representation
ranef(fm1)
```


```{julia}
# gives a NamedTuple of tables for each grouping variable
re = raneftables(fm1)
```

```{julia}
re[:dept]
```

Similarly, `condVar` and `condVartables` provide similar results for the conditional variances, which can be used to construct prediction intervals. Note that this quantity is slightly more challenging to compute, so the next code chunk can be quite slow for large and/or complex models.

```{julia}
cv = condVartables(fm1)
```

```{julia}
# this output still isn't pretty, but we're working on it!
cv[:dept]
```

At this point, it becomes convenient to place everything into a dataframe so that we can easily manipulate the relevant quantities.

```{julia}
using DataFrames
dept = DataFrame(cv[:dept])
```

Let's construct prediction intervals:

```{julia}
select!(dept, :dept, :σ => ByRow(first) => :condvar)
leftjoin!(dept, DataFrame(re[:dept]); on=:dept)
```

```{julia}
select!(dept, "dept", "(Intercept)" => "blup", "condvar")
transform!(dept,
           [:blup, :condvar] => ByRow((x,y) -> x - 1.96 * y) => :lower,
           [:blup, :condvar] => ByRow((x,y) -> x + 1.96 * y) => :upper)
```

## Measures of model fit

MixedModels.jl provides methods for the standard functions `aic`, `aicc`, `bic`, `deviance`, `fitted`, `logliklihood`, `nobs`, `residuals`.

The deviance is computed as `-2 loglikelihood` and is thus missing an additive constant for the saturated model. However, defining that constant is challenging for mixed models (what is the saturated model? do you saturate via the fixed or the random effects?) and that constant cancels out in the relevant computations.

MixedModels.jl intentionally does not provide methods for `r2` and `adjr2`. These quantities are [notoriously difficult to define in a completely satisfactory way for mixed models](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#model-summaries-goodness-of-fit-decomposition-of-variance-etc.) and we, the developers, felt uncomfortable giving our implicit endorsement by defining them as part of the core package. That said, there is an implementation of a naive definition of the coefficient of determination in [MixedModelsExtras.jl](https://palday.github.io/MixedModelsExtras.jl/v2/api/#Coefficient-of-Determination) because it is a commonly requested measure and I felt that it was better to have a well-tested implementation than have users handroll their own buggy implementation of an already problematic measure.

# Predicting new data

The `predict` function can be used to generate predictions on new data. As an initial sanity check, we can consider the case of predicting from the original data -- this should yield the fitted values:

```{julia}
predict(fm1, insteval) ≈ fitted(fm1)
```

The `predict` function supports three different options for handling new levels of the grouping variable:

- `:population`: return population values for the relevant grouping variable.
   In other words, treat the associated random effect as 0.
   If all grouping variables have new levels, then this is equivalent to
   just the fixed effects.
- `:missing`: return `missing`.
- `:error`: error on this condition.

For example, we can construct a novel dataset based on the first row of the insteval data.

```{julia}
df = first(DataFrame(insteval), 2)
df[!, :s] .= "new"
df
```

```{julia}
predict(fm1, df; new_re_levels=:population)
```

```{julia}
predict(fm1, df; new_re_levels=:missing)
```


```{julia}
#| error: true
predict(fm1, df; new_re_levels=:error)
```

Similarly, the `simulate` function can be used to simulate new data and will draw a new sample from the estimated random effects distribution:

```{julia}
using Random
simulate(MersenneTwister(42), fm1, df)
```

# Constructing more complex models

We now consider a few more complicated models to examine a few further extensions to the syntax. Including additional varying slopes is as easy as adding them before the relevant grouping variable:

```{julia}
fm2 = fit(MixedModel,
          @formula(y ~ 1 + studage + lectage + service +
                      (1 | s) +
                      (1 + service | d) +
                      (1 + service | dept)),
          insteval; progress=false)
```

```{julia}
println(fm2)
```

Of course, we want to know whether this more complicated model has a better fit than our original model. To that end, we can use the `lrtest` function (same as in GLM.jl and originally part of the StatsAPI.jl specification):

```{julia}
using GLM: lrtest
lrtest(fm1, fm2)
```

The `lrtest` function is very general and as such has rather generic output. The checks it performs for nested models are also fairly conservative in the mixed models case -- determining nesting of mixed models is not always trivial. MixedModels.jl also provides a `likelihoodratio` function that is a bit more specialized for mixed models and which performs a different, less conservative set of nesting checks:

```{julia}
using MixedModels: likelihoodratiotest
likelihoodratiotest(fm1, fm2)
```

We can see that the display is different but that the computed quantities are indeed identical.

We can also consider an even more complicated model, where there is a varying effect of `studage` by `d`ozent -- perhaps a particular instructor has a teaching style that is better for students at the beginning or end of their studies. However, `studage` is a categorical variable with four levels, so including it means that we would include three additional contrasts in the random effects. Together with the intercept and `service`, we would then have 6 * 5 / 2 = 15 correlation parameters to estimate, which dramatically increases model complexity. We can force the correlation parameters for a particular blocking variable to zero with `zerocorr`. Furthermore, if we include the same blocking variable multiple times, then the estimated correlations between the different occurrences are all forced to zero.

```{julia}
fm3 = fit(MixedModel,
          @formula(y ~ 1 + studage + lectage + service +
                      (1 | s) +
                      (1 + service | d) +
                      zerocorr(0 + studage | d) +
                      (1 + service | dept)),
          insteval; progress=false)
```

Note that correlation that are systematically zero are shown with a `.` The estimated between-department variance for the intercept term has also dropped to zero, which leads to the associated correlation being `NaN`.

```{julia}
println(fm3)
```

This additional model complexity is warranted in terms of goodness of fit:

```{julia}
likelihoodratiotest(fm2, fm3)
```

As a final note, we can also examine the effective dimensionality of the random effects with PCA [@bates:parsimonious:2018]. The property `rePCA` displays the cumulative variance explained for each principle component of each variance component and thus an estimate of excess dimensionality:

```{julia}
fm2.rePCA
```

```{julia}
fm3.rePCA
```

Together with the estimated correlations, this suggests that we could reduce the complexity of the by-department random effects. (Given that there are only 15 levels of department, it is also not surprising that we are unable to estimate subtle between department effects.)

```{julia}
fm4 = fit(MixedModel,
          @formula(y ~ 1 + studage + lectage + service +
                      (1 | s) +
                      (1 + service | d) +
                      zerocorr(0 + studage | d) +
                      (1 | dept)),
          insteval; progress=false)
println(fm4)
```

```{julia}
likelihoodratiotest(fm3, fm4)
```

```{julia}
fm4.rePCA
```

# How big can we go?

The MovieLens data [@harper:2016] contain millions of observations and provide a good stress test for model size and complexity.

:::{.callout-note}
The following models consume a large amount of memory because of the sheer size of the underlying dataset. Do **not** attempt to fit these models on a machine with less than 32GiB of memory.
:::

See also the chapter ["A large-scale observational study" in our online book *Embrace Uncertainty*](https://embraceuncertaintybook.com/largescaleobserved.html).

## Memory allocation vs. fitting

```{julia}
#| eval: false
using Econ2024
ratings = Econ2024.dataset("ratings")
@time fm_ratings = LinearMixedModel(@formula(rating ~ 1 + (1|userId) + (1|movieId)), ratings)
```

```{julia}
#| eval: false
@time fit!(fm_ratings)
```

## To try on your own after the course

```{julia}
#| eval: false
using Econ2024
ratings = DataFrame(Econ2024.dataset("ratings_genre"))
describe(ratings)
```

```{julia}
#| eval: false
using StatsBase
mcount = countmap(ratings.movieId)
ucount = countmap(ratings.userId)
mexclude = Set(k for (k, v) in pairs(mcount) if v < 50)
uexclude = Set(k for (k, v) in pairs(ucount) if v < 50)
ratings = subset(ratings,
                 :movieId => ByRow(!in(mexclude)),
                 :userId => ByRow(!in(uexclude)))
```

```{julia}
#| eval: false
# This takes about an hour on my home computer when using the full dataset
form1 = @formula(rating ~ 0 + Action + Adventure + Animation +
                             Children + Comedy + Crime +
                             Documentary + Drama +
                             Fantasy + Film_Noir +
                             Horror + IMAX +
                             Musical + Mystery + Romance +
                             Sci_Fi + Thriller + War + Western +
                             (1 | movieId) +
                             (1 | userId))
fit(MixedModel, form1, ratings)
```

<!-- # Saving and restoring fits -->

# Generalized Linear Mixed Effects Models

One of the test data sets from the Center for Multilevel Modelling, University of Bristol is derived from the 1989 Bangladesh Fertility Survey [@huq:cleland:1990].
The data are a subsample of 1934 women selected from 60 of the 64 political districts or *zila*, available as the `contra` data set in the `MixedModels` package.

Variable in the dataset:

-  `use`, whether the woman chooses to use artificial contraception, with two possible values, `N` and `Y`
- `dist`, district in which the woman resides
- `livch`, the number of live children she currently has, coded as `0`, `1`, `2`, and `3+`
- `age`, in years, but pre-centered and rounded, with the original center not available
- `urban`, coded as `N` and `Y`, indicating rural or urban.

```{julia}
contra = MixedModels.dataset(:contra)
```

In order to simplify the data a bit, we will also add a binary variable `anych` which indicates whether the woman has *any* children:

```{julia}
contra = DataFrame(contra)
contra[!, :anych] .= contra[!, :livch] .!= "0"
describe(contra)
```

::: {.callout-note}
For a more principled examination of model building with this dataset, please refer to the [chapter "Generalized linear mixed models for binary responses"](https://embraceuncertaintybook.com/glmmbernoulli.html) of *Embrace Uncertainty*.
:::

We set some appropriate contrasts

```{julia}
contrasts = Dict(:livch => HelmertCoding(; base="0"),
                 :urban => EffectsCoding(),
                 :anych => EffectsCoding())
```

and fit a model

```{julia}
gm1 = fit(MixedModel,
          @formula(use ~ 1 + urban + anych * age + abs2(age) + (1 | dist & urban)),
          contra,
          Bernoulli(),
          LogitLink(); # optional, defaults to canonical link
          nAGQ=1, # optional, default to 1
          fast=false, # optional, defaults to false, see the docs for more details.
          contrasts,
          progress=false)
```

# Limitations of MixedModels.jl

We expect that MixedModels.jl will generally be best in class for the types of models that it can fit. We use cutting edge algorithms based on penalized least squares and sparse matrix methods that take advantage of the particular sparsity and structure that arises in the case of the linear mixed effects model with an unconstrained covariance structure. Glossing over a fair number of technical details, MixedModels.jl uses a different, novel formulation of the underlying numerical problem which tends to be much more efficient computationally and allows us to fit models with multiple crossed, partially crossed or nested grouping variables without any special treatment.

## Very few options for covariance structure

Nonetheless, there is no free lunch and the tradeoff that we make is that it is *much* more difficult to formulate constraints on the covariance structure (whether on the random effects or on the response/residuals) in our formulation. MixedModels.jl currently supports precisely two covariance structures explicitly:

1. unconstrained
2. zero correlation (diagonal covariance structure)

It is also possible to express some models with compound symmetry by clever manipulation of the formula syntax (i.e. `(1+c|g)` for categorical `c` with compound symmetry is the same as `(1|g) + (1|g&c)`).

MixedModels.jl does support constraining the residual variance to known scalar value, which is useful in meta-analysis.

[Metida.jl](https://github.com/PharmCat/Metida.jl) may provide an alternative if this functionality is required (not an endorsement).

## No support for sandwich/robust variance-covariance estimators

[*This may change in the foreseeable future!*](https://github.com/JuliaStats/MixedModels.jl/pull/768)

If this would be a valuable feature, then please [file an issue](https://github.com/JuliaStats/MixedModels.jl/issues/new). Issues are prioritized by the developers' own needs and potential impact for users, so showing a large need for a feature will tend to increase its priority.

[FixedEffectsModels.jl](https://github.com/FixedEffects/FixedEffectModels.jl) may be a viable alternative (not an endorsement). It provides "fast estimation of linear models with IV and high dimensional categorical variables" and provides similar functionality to Stata's `reghdfe` and R's `lfe` and `fixest`.



## No support for generalized linear mixed models with a dispersion parameter

While MixedModels.jl does nominally support any GLM family and link function support by GLM.jl, the results for model families with a dispersion parameter (normal with non-identity link, gamma, inverse Gaussian) are known to be incorrect. The package issues a warning if you attempt to fit such models.

## No support for polytomous responses

Multinomial and ordered responses are not supported. I am unaware of a Julia package offering support for this.

## No support for regularization of the fixed effects

[HighDimMixedModels.jl](https://github.com/solislemuslab/HighDimMixedModels.jl) may provide an alternative if this functionality is required (not an endorsement).

## No support for generalized additive mixed models

Generalized additive models can be expressed a mixed model, so supporting this would require "only" adding a translation layer.

## No support for nonlinear mixed effects models

[Pumas.jl (commercial)](https://pumas.ai/our-products/products-suite/pumas) provides this (not an endorsement).


# References

::: {#refs}
:::
